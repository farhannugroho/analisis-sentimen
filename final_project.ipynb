{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install flask\n",
    "# %pip install tweet-preprocessor\n",
    "# %pip install textblob\n",
    "# %pip install sastrawi\n",
    "# %pip install emoji\n",
    "# %pip install PySastrawi\n",
    "# %pip install pandas\n",
    "# %pip install tweepy\n",
    "# %pip install seaborn\n",
    "# %pip install matplotlib\n",
    "# %pip install scikit-learn\n",
    "# %pip install wordcloud\n",
    "# %pip install apify-client\n",
    "# %pip install wordcloud\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import nltk\n",
    "import preprocessor as p\n",
    "from preprocessor.api import clean, tokenize, parse\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import emoji\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "from apify_client import ApifyClient\n",
    "from flask import Flask, make_response, request, render_template\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def home():\n",
    "    html_content = \"\"\"\n",
    "        <html>\n",
    "          <head>\n",
    "            <title>Analisis Sentimen</title>\n",
    "            <style>\n",
    "                .container {\n",
    "                    margin: 50px auto;\n",
    "                    width: 450px;\n",
    "                    text-align: center;\n",
    "                }\n",
    "\n",
    "                h1 {\n",
    "                    font-size: 24px;\n",
    "                    color: #333;\n",
    "                }\n",
    "\n",
    "                .input-area {\n",
    "                    width: 100%;\n",
    "                    height: 200px;\n",
    "                    margin-top: 20px;\n",
    "                    padding: 10px;\n",
    "                    font-size: 16px;\n",
    "                    border: 1px solid #ccc;\n",
    "                    border-radius: 5px;\n",
    "                }\n",
    "\n",
    "                .submit-button {\n",
    "                    margin-top: 20px;\n",
    "                    padding: 10px 20px;\n",
    "                    background-color: #333;\n",
    "                    color: #fff;\n",
    "                    border: none;\n",
    "                    border-radius: 5px;\n",
    "                    cursor: pointer;\n",
    "                    font-size: 16px;\n",
    "                }\n",
    "\n",
    "                .loading {\n",
    "                    display: none;\n",
    "                    margin-top: 20px;\n",
    "                }\n",
    "\n",
    "                .loading-text {\n",
    "                    font-size: 16px;\n",
    "                    color: #333;\n",
    "                }\n",
    "            </style>\n",
    "            <script>\n",
    "                function showLoading() {\n",
    "                    document.getElementById('loading').style.display = 'block';\n",
    "                }\n",
    "            </script>\n",
    "          </head>\n",
    "          <body>\n",
    "              <div class=\"container\">\n",
    "                  <h1>Input Link Posting Instagram</h1>\n",
    "                  <form action=\"/metodologi\" method=\"POST\">\n",
    "                      <textarea id=\"link\" name=\"link\" class=\"input-area\" placeholder=\"Masukkan link disini\"></textarea>\n",
    "                      <input style=\"background-color: green; color: white;\" class=\"submit-button\" onclick=\"showLoading()\" type=\"submit\">\n",
    "                  </form>\n",
    "                  <div id=\"loading\" class=\"loading\">\n",
    "                      <img src=\"https://media.tenor.com/wpSo-8CrXqUAAAAi/loading-loading-forever.gif\" alt=\"Loading\" width=\"50\"\n",
    "                          height=\"50\">\n",
    "                      <p class=\"loading-text\">Sedang Memproses...</p>\n",
    "                  </div>\n",
    "              </div>\n",
    "          </body>\n",
    "        </html>\n",
    "    \"\"\"\n",
    "    response = make_response(html_content)\n",
    "    return response\n",
    "\n",
    "@app.route('/metodologi', methods=['POST'])\n",
    "def register():\n",
    "    # apifyToken = request.form.get('apifyToken')\n",
    "    # link = request.form.get('link')\n",
    "    \n",
    "    # link = link.replace(\" \",\"\")\n",
    "    # link = link.split(\",\")\n",
    "    \n",
    "    # print(f\"apifyToken: {apifyToken}\")\n",
    "    # print(f\"link: {link}\")\n",
    "    \n",
    "    # # Initialize the ApifyClient with your API token\n",
    "    # client = ApifyClient(\"apify_api_20esNplRw082naUn9ckCu6SXMIsWcE0YCS7h\")\n",
    "\n",
    "    # # Prepare the actor input\n",
    "    # run_input = {\n",
    "    #   \"directUrls\": link,\n",
    "    #   \"resultsLimit\": 1500,\n",
    "    # }\n",
    "    \n",
    "    # print(run_input)\n",
    "    \n",
    "    # # Run the actor and wait for it to finish\n",
    "    # run = client.actor(\"apify/instagram-comment-scraper\").call(run_input=run_input)\n",
    "    \n",
    "    # db_comment = pd.DataFrame(columns=[\"userId\",\"createdAt\",\"text\"])\n",
    "    # for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    #     value1 = item[\"id\"]\n",
    "    #     value2 = item[\"timestamp\"]\n",
    "    #     value3 = item[\"text\"]\n",
    "    \n",
    "    #     # Create a new row with the values\n",
    "    #     new_row = {\"userId\": value1, \"createdAt\": value2,\"text\": value3}\n",
    "    \n",
    "    #     # Append the new row to the DataFrame\n",
    "    #     db_comment = pd.concat([db_comment, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "    \n",
    "    # filename = \"data-instagram.csv\"\n",
    "    # db_comment.to_csv(filename, index=False)\n",
    "    \n",
    "    # ######################################################################################\n",
    "    # ################################ PREPROCESSING DATA ##################################\n",
    "    # ######################################################################################\n",
    "    \n",
    "    #Read Data\n",
    "    def load_data():\n",
    "      data = pd.read_csv('data-instagram.csv')\n",
    "      return data\n",
    "    \n",
    "    comment_df = load_data()\n",
    "    comment_df = pd.DataFrame(comment_df[['userId', 'createdAt', 'text']])\n",
    "    print(comment_df.head(1500))\n",
    "\n",
    "    #cleaning\n",
    "    def remove_pattern(text, pattern_regex):\n",
    "      r = re.findall(pattern_regex, text)\n",
    "      for i in r:\n",
    "        text = re.sub(i, '', text)\n",
    "        return text\n",
    "      \n",
    "    # remove tagging\n",
    "    comment_df = comment_df[comment_df['text'].notnull()]\n",
    "    comment_df['clean_tagging'] = np.vectorize(remove_pattern)(comment_df['text'], \" *RT* | *@[\\w]*\")\n",
    "    print(comment_df.head(10))\n",
    "    \n",
    "    #remove emoji & character\n",
    "    def remove(text):\n",
    "      text =' '.join(re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "      return text\n",
    "    comment_df['remove_character'] = comment_df['text'].apply(lambda x: remove(x))\n",
    "    print(comment_df.head(9))\n",
    "    \n",
    "    #remove hastag\n",
    "    def remov(text):\n",
    "      text = re.sub(r'r\\$\\w*', '', text)\n",
    "      text = re.sub(r'^RT[\\s]+', '', text)\n",
    "      text = re.sub(r'#', '', text)\n",
    "      text = re.sub(r'[0-9]+', '',text)\n",
    "\n",
    "      return text\n",
    "\n",
    "    comment_df['remove_hastag'] = comment_df['remove_character'].apply(lambda x: remov(x))\n",
    "    print(comment_df.head(10))\n",
    "    \n",
    "    #remove duplikat\n",
    "    comment_df.drop_duplicates(subset = \"remove_hastag\", keep = 'first', inplace = True)\n",
    "    print(comment_df.head(10))\n",
    "    \n",
    "    #import stopword\n",
    "    nltk.download('stopwords')\n",
    "    stopwords_indonesia = stopwords.words('indonesian')\n",
    "    stopwords_indonesia\n",
    "\n",
    "    stop_factory = StopWordRemoverFactory().get_stop_words()\n",
    "    more_stopwords = [\n",
    "        'yg', 'utk', 'cuman', 'deh', 'Btw', 'tapi', 'gua', 'gue', 'lo', 'lu',\n",
    "        'kalo', 'trs', 'jd', 'nih', 'ntr', 'nya', 'lg', 'gk', 'ecusli', 'dpt',\n",
    "        'dr', 'kpn', 'kok', 'kyk', 'donk', 'yah', 'u', 'ya', 'ga', 'km', 'eh',\n",
    "        'sih', 'eh', 'bang', 'br', 'kyk', 'rp', 'jt', 'kan', 'gpp', 'sm', 'usah'\n",
    "        'mas', 'sob', 'thx', 'ato', 'jg', 'gw', 'wkwkwk', 'mak', 'haha', 'iy', 'k'\n",
    "        'tp','haha', 'dg', 'dri', 'duh', 'ye', 'wkwk', 'syg', 'btw',\n",
    "        'nerjemahin', 'gaes', 'guys', 'moga', 'kmrn', 'nemu', 'yukk',\n",
    "        'wkwkw', 'klas', 'iw', 'ew', 'lho', 'sbnry', 'org', 'gtu', 'bwt',\n",
    "        'krlga', 'clau', 'lbh', 'cpet', 'ku', 'wke', 'mba', 'mas', 'sdh', 'kmrn',\n",
    "        'oi', 'spt', 'dlm', 'bs', 'krn', 'jgn', 'sapa', 'spt', 'sh', 'wakakaka',\n",
    "        'sihhh', 'hehe', 'ih', 'dgn', 'la', 'kl', 'ttg', 'mana', 'kmna', 'kmn',\n",
    "        'tdk', 'tuh', 'dah', 'kek', 'ko', 'pls', 'bbrp', 'pd', 'mah', 'dhhh',\n",
    "        'kpd', 'tuh', 'kzl', 'byar', 'si', 'sii', 'cm', 'sy', 'hahahaha', 'weh',\n",
    "        'dlu', 'tuhh'\n",
    "    ]\n",
    "    data = stop_factory + more_stopwords\n",
    "\n",
    "    dictionary = ArrayDictionary(data)\n",
    "    stopWord = StopWordRemover(dictionary)\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    #import Sastrawi\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "    #tokenize\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "    #Happy Emoticon\n",
    "    emoticons_happy = set([\n",
    "        ':-)', ':)', ';)', ':o', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "        ':^)', ':-D', ':D', '8-D', '8D', ',x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "        '=-3', '=3', ':-))', \":'-)\", \":')\", ':*)', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "        'x-P', 'xp', ' XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "        '<3' \n",
    "    ])\n",
    "\n",
    "    #Sad emoticon\n",
    "    emoticons_sad = set([\n",
    "        ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "        ':-[', ':-<', '=\\\\', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "      ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "    #all emtoicons (happy + sad)\n",
    "    emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "    def clean_comment(comment):\n",
    "      #tokenize\n",
    "      tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "      comment_tokens = tokenizer.tokenize(comment)\n",
    "\n",
    "      comments_clean = []\n",
    "      for word in comment_tokens:\n",
    "        if (\n",
    "            word not in data and\n",
    "            word not in emoticons and\n",
    "            word not in string.punctuation):\n",
    "            stem_word = stemmer.stem(word)\n",
    "            comments_clean.append(stem_word)\n",
    "\n",
    "      return comments_clean\n",
    "    comment_df['clean_comment'] = comment_df ['remove_hastag'].apply(lambda x:clean_comment(x))\n",
    "    #tokenization\n",
    "    print(comment_df.head(10))\n",
    "    \n",
    "    #remove punct\n",
    "    def remove_punct(text):\n",
    "      text = \" \".join([char for char in text if char not in string.punctuation])\n",
    "      return text\n",
    "    comment_df['clean_comment'] = comment_df ['clean_comment'].apply(lambda x:remove_punct(x))\n",
    "    print(comment_df.head(10))\n",
    "    \n",
    "    #reset index\n",
    "    comment_df = comment_df.reset_index(drop=True)\n",
    "    print(comment_df.head(10))\n",
    "    \n",
    "    comment_df.drop_duplicates(subset =\"remove_hastag\", keep = 'first', inplace = True)\n",
    "    print(comment_df.head(10))\n",
    "    \n",
    "    #remove kolom\n",
    "    comment_df.drop(comment_df.columns[[0,1,2,3,4,5]], axis = 1, inplace = True)\n",
    "    print(comment_df.head(1500))\n",
    "    \n",
    "    #simpan data bersih\n",
    "    comment_df.to_csv('komentar_bersih_instagram.csv', encoding ='utf8', index = False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"######################################################################################\")\n",
    "    print(\"################################## WEIGHT SENTIMENT ##################################\")\n",
    "    print(\"######################################################################################\")\n",
    "    \n",
    "    df = pd.read_csv('komentar_bersih_instagram.csv', usecols=['clean_comment']).astype('str')\n",
    "    print(df.head(10))\n",
    "    \n",
    "    # LEXICON\n",
    "    lexicon = pd.read_csv('lexicon.csv')\n",
    "    lexicon['weight'] = lexicon['sentiment'].map({'positive':1, 'negative':-1}) \n",
    "    lexicon = dict(zip(lexicon['word'], lexicon['weight']))\n",
    "    print(lexicon)\n",
    "    \n",
    "    # NEGATIVE WORDS\n",
    "    negative_words = list(open(\"negative.txt\"))\n",
    "    negative_words = list([word.rstrip() for word in negative_words])\n",
    "    print(negative_words)\n",
    "    \n",
    "    comment_polarity = [] \n",
    "    comment_weight = []\n",
    "    negasi = False\n",
    "\n",
    "    for sentence in df['clean_comment']: \n",
    "      sentence_score = 0 \n",
    "      sentence_weight = \"\" \n",
    "      sentiment_count = 0 \n",
    "      sentence = sentence.split()\n",
    "      for word in sentence:\n",
    "        try:\n",
    "          score = lexicon[word]\n",
    "          sentiment_count = sentiment_count + 1\n",
    "        except:\n",
    "          score = 99\n",
    "    \n",
    "        if(score == 99):\n",
    "          if (word in negative_words): \n",
    "            negasi = True\n",
    "            sentence_score = sentence_score - 1\n",
    "            sentence_weight = sentence_weight + \" - 1\"\n",
    "          else:\n",
    "            sentence_score = sentence_score + 0 \n",
    "            sentence_weight = sentence_weight + \" + 0\"\n",
    "        else:\n",
    "          if(negasi == True):\n",
    "            sentence_score = sentence_score + (score * -1.0)\n",
    "            sentence_weight = sentence_weight + \" + (\"+ str(score) + \" * -1 \"+\") \" \n",
    "            negasi = False\n",
    "          else:\n",
    "            sentence_score = sentence_score + score \n",
    "            sentence_weight = sentence_weight + \" + \"+ str(score)\n",
    "        \n",
    "      comment_weight.append(sentence_weight[1:] +\" = \" + str(sentence_score)) \n",
    "      if sentence_score > 0:\n",
    "        comment_polarity.append('positive') \n",
    "      elif sentence_score < 0:\n",
    "        comment_polarity.append('negative') \n",
    "      else:\n",
    "        comment_polarity.append('neutral') \n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        \"comment\" : df['clean_comment'], \n",
    "        \"label\" : comment_polarity, \n",
    "        \"weight\" : comment_weight\n",
    "        })\n",
    "    results['label'].value_counts()\n",
    "    results[['comment', 'label']].to_csv('labeling-data-instagram.csv', encoding ='utf8', index = False)\n",
    "    print(results.head(20))\n",
    "    \n",
    "    \n",
    "    print(\"######################################################################################\")\n",
    "    print(\"#################################### DATA MINING #####################################\")\n",
    "    print(\"######################################################################################\")\n",
    "\n",
    "    df = pd.read_csv('labeling-data-instagram.csv', usecols=['comment', 'label']).dropna() \n",
    "\n",
    "    tf = TfidfVectorizer()\n",
    "    text_tf = tf.fit_transform(df['comment'])\n",
    "\n",
    "    temporary_df = pd.DataFrame(text_tf.todense(), columns=tf.get_feature_names_out())\n",
    "    temporary_df\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train,\tX_test,\ty_train,\ty_test\t=\ttrain_test_split(text_tf,\tdf['label'],\ttest_size=0.1, random_state=42)\n",
    "    \n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from\tsklearn.metrics\timport\taccuracy_score,\tprecision_score, recall_score, f1_score\n",
    "    from\tsklearn.metrics\timport classification_report \n",
    "    from\tsklearn.metrics\timport confusion_matrix\n",
    "\n",
    "    clf = MultinomialNB().fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_test)\n",
    "    # print(\"Multinomial NB Accuracy  : \", accuracy_score(y_test,predicted))\n",
    "    # print(\"Multinomial NB Precision : \", precision_score(y_test,predicted, average = 'macro', pos_label=\"Positif\"))\n",
    "    # print(\"Multinomial NB Recall    : \", recall_score(y_test,predicted, average = 'macro', pos_label=\"Positif\"))\n",
    "    # print(\"Multinomial NB F-Measure : \", f1_score(y_test,predicted, average = 'macro', pos_label=\"Positif\"))\n",
    "    # print(classification_report(y_test, predicted, zero_division=0))    \n",
    "    \n",
    "    # Perform predictions and calculations\n",
    "    accuracy = accuracy_score(y_test, predicted)\n",
    "    precision = precision_score(y_test, predicted, average='macro', pos_label=\"Positif\")\n",
    "    recall = recall_score(y_test, predicted, average='macro', pos_label=\"Positif\")\n",
    "    f_measure = f1_score(y_test, predicted, average='macro', pos_label=\"Positif\")\n",
    "    classification = classification_report(y_test, predicted, zero_division=0)\n",
    "    \n",
    "    \n",
    "    import seaborn as sns \n",
    "    import matplotlib.pyplot as plt \n",
    "    cm = confusion_matrix(predicted, y_test) \n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.title('Confusion Matrix') \n",
    "    plt.savefig('static/confusion-matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    predicted_proba = clf.predict_proba(X_test)\n",
    "    predicted_proba\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "    print(\n",
    "        f'ROC\tAUC\tscore:  {roc_auc_score(y_test, predicted_proba[:,\t:], multi_class=\"ovr\").round(4)}')\n",
    "    \n",
    "    y_test_bin = label_binarize(y_test, classes=['negative', 'neutral', 'positive']) \n",
    "    n_classes = y_test_bin.shape[1]\n",
    "    fpr = dict() \n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    roc_auc\n",
    "    colors = ['violet', 'black', 'yellow'] \n",
    "    auc_scores = []\n",
    "    for i in range(n_classes):\n",
    "      fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], predicted_proba[:, i]) \n",
    "      plt.plot(fpr[i], tpr[i], color=colors[i], lw=2, label= str(i)) \n",
    "      # print('AUC for Class {}: {}'.format(i, auc(fpr[i], tpr[i]).round(3)))\n",
    "      # print()\n",
    "      auc_score = auc(fpr[i], tpr[i]).round(3)\n",
    "      auc_scores.append(auc_score)\n",
    "      \n",
    "      \n",
    "    # plt.figure(figsize=(12, 8))\n",
    "    plt.plot([0, 1], [0,1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05]) \n",
    "    plt.xlabel('False Positive Rate') \n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic Curves') \n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig('static/grafic-roc.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    isi_text = df['comment']\n",
    "    isi_text\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    from wordcloud import WordCloud\n",
    "\n",
    "    #convert list to string and generate\n",
    "    unique_string=(\" \").join(df['comment'])\n",
    "    wordcloud = WordCloud(width = 1000, height = 500,background_color ='white').generate(unique_string)\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"static/wrcld.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()  \n",
    "    \n",
    "    csv_files = ['data-instagram.csv', 'komentar_bersih_instagram.csv', 'labeling-data-instagram.csv']\n",
    "    data = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file)\n",
    "        df = df.head(10)\n",
    "        # Convert the DataFrame to a dictionary and append it to the data list\n",
    "        data.append(df.to_dict('records'))\n",
    "        \n",
    "    return render_template('index.html', data=data,accuracy=accuracy, precision=precision, recall=recall, f_measure=f_measure, classification=classification, auc_scores=auc_scores)\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
